Based on the images provided, here is the translation of the text and a detailed explanation of the charts.

---

### **Slide 1: Title Screen**

**Text Translation:**
*   **Top Left:** PwC
*   **Main Title:** AI Management Endowed Chair
*   **Subtitle:** AI Business Insights 2026
*   **Bottom Line:** Introductory Lecture Materials

---

### **Slide 2: Introduction of Lecturers**

**Text Translation:**

**Shinichiro Sanji**
*   **Role:** Director, Technology Laboratory, PwC Consulting LLC.
*   **Bio:** After working at a major Japanese think tank and a consulting firm, he assumed his current position. He has led projects based on advanced technologies, including robotics, across industry, government, and academia. He plays a role in connecting enterprises, government agencies, industry bodies, and academia. His strength lies in consulting that leverages the unique characteristics of each sector (industry, government, academia). He provides high-level consulting ranging from policy-making support and technical evaluation of research institutions to implementation support for new business ventures in companies.

**Atsushi Komeya**
*   **Role:** Technology Laboratory, PwC Consulting LLC.
*   **Bio:** Engaged in AI-related R&D at a major telecommunications company, promoting joint projects with companies and industry-academia collaboration activities. Later, at a US strategic firm, he engaged in business strategy and operational transformation in the finance and public sectors. Currently, he leads the Physical AI area, supporting new business concepts centered on AI and Robotics, developing in-house AI products, and engaging in industry-academia collaborative activities such as joint research and providing lectures.

---

### **Slide 3: AI Management Endowed Chair (Program Overview)**

**Text Translation:**
*   **PwC Japan Group** advocates "AI Management," where companies incorporate data utilization based on AI into the core of their management. We support improving corporate competitiveness by ultimately embedding AI into all business operations.
*   **The University of Tokyo (Matsuo/Iwasawa Lab)** is building an ecosystem of research, education, social implementation, and incubation to realize the mission of nurturing "pioneers" of the era, changing the times, and opening up the future.
*   **Agreement:** Both parties agreed to nurture management talent who can enhance Japan's competitive advantage and create the future through the use of AI and digital technologies, leading to the establishment of this course.
*   **Sponsorship:** This term is also opened as an endowed course by PwC Japan Group and Itoki Corporation.

---

### **Slide 4: Current State of AI Utilization (Chart Analysis)**

**Headline Text:**
"The use of Generative AI has expanded dramatically in a short period and is becoming a foundation of management. On the other hand, organizations already proceeding with utilization are seeing a **polarization**: those achieving results as expected and those falling below expectations. The importance of initiatives toward effective utilization is increasing."

#### **Chart 1 (Left): "Progress of Generative AI Utilization in Companies"**

*   **Overview:** This stacked bar chart tracks the adoption rate of Generative AI in companies over three years (Spring 2023 to Spring 2025).
*   **The Trend:** Adoption is skyrocketing.
    *   **Spring 2023:** Only **22%** were involved (8% Using + 14% Promoting). A massive 78% had not started or were only considering it.
    *   **Spring 2024:** Involvement jumped to **67%** (43% Using + 24% Promoting).
    *   **Spring 2025:** Involvement reached **76%** (56% Using + 20% Promoting).
*   **Key Takeaway:** As noted by the callout box, **"About 80% are promoting Generative AI utilization."** The phase of "wondering whether to use it" is effectively over; the market has shifted to implementation.

#### **Chart 2 (Right): "Gap between Expectations and Results of Generative AI"**

*   **Overview:** This chart compares survey results from Spring 2024 to Spring 2025 regarding how well AI performance met business expectations.
*   **The Data:**
    *   **Positive (Orange):** The percentage of companies saying results "Greatly exceeded" or "Met" expectations remained stable at roughly **60%** (9%+48% vs 10%+51%).
    *   **Negative (Red/Pink):** The percentage of companies saying results were "Slightly below" or "Far below" expectations **increased by 7 points** (from roughly 18% to 25%).
    *   **Uncertain (Brown):** The number of companies who "Cannot evaluate yet" dropped significantly (25% to 14%).
*   **Analysis:**
    *   As companies moved from "Testing" (the 'Cannot evaluate' group) to actual "Implementation," a significant portion realized it was harder than expected.
    *   This indicates the **"Polarization"** mentioned in the headline. While the majority (60%) are satisfied, a growing minority (1 in 4 companies) are finding that AI is underperforming against their initial hype or expectations. This suggests that simply *having* the tool is not enough; *how* it is managed determines success.

 
 "

---

### **Slide 5: Features of the AI Management Endowed Chair**

**Headline:**
**Features of the AI Management Endowed Chair**

**Introductory Text:**
"The purpose of this course is to utilize AI in management. Beyond just the potential of AI, students can deepen their understanding from a broad perspective, including risks and challenges that must be understood, as well as use cases and enablers for promoting AI management more effectively. We deliver the ideal form of management in the AI era along with the raw voices of lecturers who are active on the front lines of business."

**Key Points (Three Icons):**

1.  **Learn Business Innovation opened up by Generative AI and Physical AI**
    *   **Translation:** "Generative AI is utilized not only for operational efficiency but also for promoting creation and innovation. Furthermore, 'Physical AI,' which works in real-world spaces (like AI Agents), is gaining attention. These technologies connect the digital and physical spaces, holding the potential to drastically change industrial structures and business models."
    *   **Explanation:** The course isn't just about chatbots; it covers "Physical AI" (robots/agents acting in the real world), signaling a shift from screen-based AI to action-based AI.

2.  **Promoting Understanding of Risks associated with AI Utilization**
    *   **Translation:** "AI presents ethical challenges and new risks, requiring high-level talent for appropriate management. However, Japanese companies face major challenges in talent development for business promotion using AI due to a lack of AI literacy within the company and undeveloped organizational structures."
    *   **Explanation:** This highlights a defensive strategy. It acknowledges that Japan lacks the internal talent to govern AI properly, and this course aims to fill that gap.

3.  **Developing Talent that Integrates Technology and Management**
    *   **Translation:** "In Japan, there is a shortage of talent who can manage both technology and business management, acting as a barrier to the effective use of solutions including AI. The AI Management Endowed Chair aims to nurture management talent who can integrate technology and business, supporting the improvement of Japanese companies' global competitiveness."
    *   **Explanation:** The core mission is to bridge the "Tech" vs. "Business" divide, creating "bilingual" leaders who speak both languages.

---

### **Slide 6: Achievements up to Last Year**

**Headline:**
**Achievements up to Last Year**

**Sub-headline:**
"In FY2024, a record 6,830 students participated. The course structure and lecturers are updated every year."

**Chart Analysis (Attendance Growth):**
*   **FY2021:** 308 Students
*   **FY2022:** 351 Students
*   **FY2023:** 580 Students
*   **FY2024:** **6,830 Students**
*   **Insight:** There was an **11x increase** in attendance between 2023 and 2024. This explosion in interest directly correlates with the "Generative AI Boom" (ChatGPT era).

**Curriculum Evolution (Themes):**
*   **2021 (Foundations):** Opening, Current State of AI Tech, EC (E-commerce), Finance, Disruption, Mobility, Governance.
*   **2022 (Expansion):** AI Management for CXOs, Healthcare, Digital Twins, Insurance & Retail case studies.
*   **2023 (Industry Focus):** Logistics, Manufacturing, Finance, Insurance case studies.
*   **2024 (The GenAI Era):**
    1.  Opening
    2.  **Evolution of Generative AI** & Future Prospects
    3.  Importance of AI **Governance** & Practice
    4.  AI & Sustainability
    5.  Customer Experience (CX) & Marketing transformed by AI
    6.  Co-evolution of AI and **Robotics**
    7.  Back-office business transformed by AI
    8.  Talent Development in the AI Era
    9.  Co-creation Strategy in the AI Era
    10. Closing

---

### **Slide 7: Course Structure & Positioning of "Introduction"**

**Headline:**
**Overall Course Structure and Positioning of "Introduction - AI Primer -"**

**Text:**
"This course features a curriculum to learn the front lines of AI utilization cross-sectionally, covering everything from overall strategy to business functions and back-office operations. This pre-learning material (Introduction) aims to organize concepts regarding AI and the basics of technology, allowing for a deeper understanding of the main course content that follows."

**Diagram Analysis (The "House" of AI Management):**
This diagram shows how the course is architected, similar to a business structure.

*   **The Roof (Strategy):**
    *   1. Business Strategy
    *   2. Technology Strategy
*   **The Pillars (Business Functions - Value Chain):**
    *   3. Planning/Development
    *   4. Procurement/Purchasing
    *   5. Manufacturing/Logistics
    *   6. Marketing/Sales
    *   7. Customer Support
*   **The Foundation (Back Office):**
    *   8. Tax / Finance / Audit
    *   9. Human Resources (Human Capital) / Organization
*   **The Bottom Layer (Synthesis & Prep):**
    *   10. Closing: Management in the AI Era
    *   **0. Introduction ~ AI Primer** (Highlighted in Grey)
*   **Explanation:** The current video is "Layer 0." It is the prerequisite knowledge needed before students split off into learning about specific departments like Marketing or HR.

---

### **Slide 8: Updates from Last Year**

**Headline:**
**Updates from Last Year's AI Management Endowed Chair**

**Introductory Text:**
"While catching up on the latest trends since last year, we cover corporate functions comprehensively. In addition to distributing pre-learning materials to learn AI basics, we will hold 'Skill Sessions' (2 spin-off events) after the lectures to explain how to use AI-powered tools, enabling deeper learning."

**Detailed Points:**

1.  **Content Based on Latest Trends**
    *   **Detail:** Lecturers are experts who understand the latest currents like **AI Agents** and **Physical AI**. They explain from a practical perspective, including onsite examples and utilization tips.

2.  **Comprehensively Covering Corporate Functions**
    *   **Detail:** In addition to themes that were popular last time, this year is structured to comprehensively cover functions within a company. Students deepen knowledge of individual functions while learning the key points of functional design looking at the whole company.

3.  **Addition of Pre-learning Materials**
    *   **Detail:** To ensure smooth participation in the main course, we are distributing the "Introduction - AI Primer" materials beforehand to organize the basics of AI concepts and technology.

4.  **Implementation of Spin-offs**
    *   **Detail:** Moving from **"Knowing" to "Using."** In a total of 2 spin-off "Skill Sessions," we introduce how to use tools that allow you to intuitively generate code using Generative AI.
    *   **Insight:** This addresses a common criticism of business courses (too much theory, not enough practice). They are adding hands-on coding/tool sessions.
  
  

---

### **Slide 9: Content of the Main Course (The 10 Sessions)**

**Headline:**
**Course Main Content**
"Learn the front lines of AI utilization comprehensively cross-sectionally, from overall strategy to business functions and back-office operations."

**Table Breakdown:**
The table lists the 10 themes for the upcoming full course, along with the difficulty level (Beginner, Intermediate, Advanced).

1.  **Business Strategy in the AI Era:** Overview of the history of AI management, impact on industry/society, and future business opportunities. *(Beginner/Intermediate)*
2.  **Technology Strategy in the AI Era:** Learning about strategies capturing tech trends like **Physical AI** and **Edge AI**, and future developments. *(Note: This is the only one marked with an "Advanced" red block, indicating it covers complex technical trends).*
3.  **Planning & Development Transformed by AI:** Learning about accelerating software dev cycles, AI governance for trust/safety, and future offensive/defensive planning.
4.  **AI-Driven Procurement & Purchasing Reform:** Learning AI-driven procurement that enables the redesign of decision-making and execution.
5.  **Operational Reform in Manufacturing via AI:** Learning AI utilization on the manufacturing floor and envisioning future operations.
6.  **Marketing & Sales Reform Sophisticated by AI Agents:** Learning the current state of AI to strengthen both customer experience and revenue, from marketing to sales.
7.  **Customer Support Reform Utilizing AI:** Thinking about future knowledge management, moving from AI-assisted communication to full knowledge management.
8.  **Tax, Finance, and Audit Evolving with AI:** Learning corporate finance and audit using AI, envisioning a future where trust infrastructure is strengthened by AI.
9.  **Talent Development & Organizational Management in the AI Era:** Learning about HR and org management with AI utilization as a premise.
10. **Management in the AI Era (Closing):** Reviewing the course from strategy to the front lines and summarizing the ideal form of management in the AI era.

---

### **Slide 11: Agenda for This Pre-learning Material (Introduction)**

**Headline:**
**Agenda for This Pre-learning Material (Introduction)**

**Sub-headline:**
"We explain content mainly aimed at beginners, such as basic concepts for each theme and explanations of terminology that will help you understand the course better."

**Agenda Items:**

1.  **Use Cases of AI Utilization**
    *   **Content:** Learn use cases at the front lines of AI utilization and grasp the image of how it is used.
    *   **Difficulty:** Beginner

2.  **AI Overview (Broken into 4 parts)**
    *   **Traditional AI:** Explanation of AI and mechanisms (like Machine Learning) before the rapid evolution of Generative AI.
    *   **Generative AI:** Basics and mechanisms of GenAI (LLMs, etc.), explaining the difference between general-purpose and specialized types.
    *   **AI Agents:** Explanation of the basics of AI Agents, where multiple GenAIs interact complexly to execute tasks autonomously.
    *   **Physical AI / Embodied AI:** Explanation of the basics and capabilities of Physical AI, which connects physical space (robots, etc.) with AI in cyber space. *(Includes Intermediate difficulty)*

3.  **Course Curriculum**
    *   **Content:** Explanation of the flow of all 10 sessions and points to keep in mind when taking the course.

---

### **Slide 12: Section Title**

**Text:**
**AI Utilization Use Cases**

*Context: The speaker is moving into the first agenda item listed in the previous slide.*

---

### **Slide 13: Categories of AI Use Cases**

**Headline:**
**AI Utilization Use Cases**

**Sub-text:**
"The utilization of Generative AI is diverse, covering text, program code, image/video, and voice/music generation."

**The 5 Categories (Icons):**

1.  **Text**
    *   Generating marketing copy, email generation, chatbot support, creating meeting minutes, document summarization/proofreading, etc.
2.  **Program Code**
    *   Program generation, documentation, Text-to-SQL (turning questions into database queries), Web App generation, etc.
3.  **Image / Video**
    *   Marketing content, Web design, generating medical images for rare diseases (synthetic data), style transfer, etc.
4.  **Voice / Music**
    *   Presentation voice synthesis, music generation, etc.
5.  **Other**
    *   3D Model (Avatar) generation, startup/entrepreneurial support, etc.

---

### **Slide 14: Deep Dive into Text Generation**

**Headline:**
**Example of Text Generation Use Case**

**Sub-text:**
"Text generation is not limited to drafting minutes or emails; the scope of utilization is expanding to **automation of reports**."

**Two Key Examples:**

**Left: Automatic Generation of Research Reports**
*   **Description:** Collecting and organizing web information, papers, and materials cross-sectionally. It organizes arguments, structures them, and automatically generates a report **with citations**.
*   **Visual:** Shows a ChatGPT-like interface conducting "Deep research" and citing sources (e.g., reading from britannica.com, wikimedia.org).

**Right: Automatic Creation of Reports (Business Docs)**
*   **Description:** Based on meeting minutes, past materials, and internal knowledge (company wiki), it automatically creates consistent business documents ranging from background organization to conclusions.
*   **Visual:** Shows a workspace tool (likely Notion AI or similar) creating a "Bug Tracking Dashboard" and documentation automatically.

**Key Insight:** The speaker is emphasizing that text AI has moved beyond simple "chatting" to complex **synthesis** tasks—taking vast amounts of external data (left side) or internal data (right side) and structuring it into final business deliverables.



---

### **Slide 15: Program Code Generation Use Cases**

**Headline:**
**Example of Program Code Generation Use Case**

**Sub-text:**
"Program generation use cases are diverse, ranging from automatic code generation from natural language instructions to the automation of creating and executing test cases. Furthermore, AI utilization is spreading to the entire development process."

**Two Key Examples:**

**Left: No-Code Development Tools**
*   **Description:** "Implementing AI Agents with an intuitive interface."
*   **Visual:** The screenshot shows **Dify**, a popular open-source platform for building AI apps. It displays a visual flow-chart where a user connects blocks (Start -> Extract Data -> Answer) without writing raw code.

**Right: Test Automation**
*   **Description:** "Extracting test cases or generating test code via low-code."
*   **Visual:** The screenshot shows **UiPath Studio**, a leading RPA (Robotic Process Automation) tool. It illustrates how AI can automatically generate the scripts needed to test software, reducing the manual burden on QA teams.

---

### **Slide 16: Image / Video Use Cases**

**Headline:**
**Example of Image / Video Use Case**

**Sub-text:**
"There are use cases where you can easily edit images to match a specific tone starting from AI image generation, or generate videos from short instruction sentences."

**Two Key Examples:**

**Left: Image Creation**
*   **Description:** "Generating AI images or appropriate text that matches the tone of the image."
*   **Visual:** Shows **Google Gemini**. The example features a 360-degree wide-angle shot of a dog, demonstrating the high fidelity and specific camera-lens effects AI can now produce.

**Right: Video Creation**
*   **Description:** "Generating video that maintains composition, movement, and **temporal consistency** from short instruction sentences."
*   **Visual:**
    *   **Runway:** A high-end AI video editor used for professional creative work.
    *   **Adobe Firefly:** Shows the "Text to Video" feature, emphasizing how commercial tools are integrating these capabilities.

---

### **Slide 17: Voice / Music Use Cases**

**Headline:**
**Example of Voice / Music Use Case**

**Sub-text:**
"There are use cases enabling automatic generation of natural speech by AI, or automatic generation of music matching an image just by specifying a genre."

**Two Key Examples:**

**Left: Automatic Generation of Natural Speech**
*   **Description:** "Automatically generating human-like voice including emotion and intonation."
*   **Visual:** Shows **ElevenLabs**, widely considered the industry standard for realistic AI voice synthesis. It highlights the ability to clone voices or create specific character voices.

**Right: Automatic Generation of Music**
*   **Description:** "Generating a whole song with vocals just by specifying a genre."
*   **Visual:**
    *   **Soundraw:** A tool often used for background music generation.
    *   **Suno:** A popular tool that generates full lyrical songs. The prompt window shows "Choose a genre," illustrating the ease of use for non-musicians.

---

### **Slide 18: Section Title**

**Text:**
**AI Overview**

*Context: The presentation now shifts from practical examples (Use Cases) to theoretical frameworks.*

---

### **Slide 19: Classification of AI (Important Concept)**

**Headline:**
**Classification of AI**

**Sub-text:**
"With the development and spread of not only Traditional AI but also Generative AI and AI Agents, utilization is beginning to be trialed in all scenarios."

**The Three Stages of AI Evolution (Left to Right):**

1.  **Traditional AI (Discriminative / Predictive AI)**
    *   **Role:** AI that **Recognizes or Predicts**.
    *   **Process:** Input (Data) $\rightarrow$ Processing (Machine Learning / Deep Learning) $\rightarrow$ Output (Voice Recognition, Text Recognition, Numerical Prediction).
    *   **Key:** It analyzes *existing* data to classify it or predict a future number.

2.  **Generative AI**
    *   **Role:** AI that **Creates Content**.
    *   **Process:** Input (Question / Prompt) $\rightarrow$ Processing (LLM - Large Language Model, etc.) $\rightarrow$ Output (Answer, Text writing, Image generation, Voice synthesis).
    *   **Key:** It creates *new* data that didn't exist before.

3.  **AI Agent (The Next Frontier)**
    *   **Role:** AI that **Thinks, Acts, and Learns Autonomously**.
    *   **Process:**
        *   Input: **Goal**
        *   **The Agent Cycle:**
            1.  **Plan:** Determine how to solve the problem.
            2.  **Action:** Execute the necessary steps (e.g., browse the web, run code).
            3.  **Review:** Check the results of the action.
            4.  **Re-plan:** If the result wasn't perfect, adjust the plan.
        *   Output: **Goal Achievement**
    *   **Key:** Unlike Generative AI which waits for a user prompt for every single step, an Agent is given a broader goal and loops through a cycle of planning and acting until it finishes the job.
  

---

### **Slide 20: The Transition (History) of AI**

**Headline:**
**Transition of AI**

**Summary Text:**
"AI has repeated cycles of 'Booms' and 'Winters' (periods of stagnation). However, due to (1) Improved Machine Power, (2) Increase in Data Volume, and (3) Evolution of Deep Learning, we entered a massive 3rd Boom. Furthermore, we are now said to be in the **4th Boom** due to the emergence and practical application of Generative AI."

**The Timeline Graph (Left):**
*   **1st AI Boom (1960s):** Focused on "Search and Games" (like solving mazes).
    *   *The Winter:* It stopped because it could only solve simple problems.
*   **2nd AI Boom (1980s):** Focused on "Expert Systems" (teaching expert knowledge to computers).
    *   *The Winter:* It stopped due to technical, cost, and time constraints in inputting/maintaining all that knowledge.
*   **3rd AI Boom (2010s):** Focused on "Machine Learning / Image Recognition."
    *   *The Shift:* Rapid practical application.
*   **4th AI Boom (2020s - Present):** Focused on "Generative AI" (Text, Image, Video, Code generation). The curve shoots straight up.

**The Drivers of the Recent Boom (Right Panel):**
1.  **Improvement of Machine Power:** Widespread use of chips specialized for AI (GPUs) and utilization of computing resources in the Cloud.
2.  **Increase in Data Volume:** In the 10 years from 2010 to 2020, data volume increased **40-fold**. The availability of digital data contributed to AI development.
3.  **Evolution of Deep Learning:** Professor Hinton (University of Toronto) proved the effectiveness of Deep Learning. The programming language Python spread, and various companies released Deep Learning libraries.

---

### **Slide 21: Section Title**

**Text:**
**Traditional AI**

*Context: The presentation now defines "Traditional AI" to distinguish it from the new "Generative AI."*

---

### **Slide 22: Traditional AI (The 4 Categories)**

**Headline:**
**Traditional AI**

**Sub-headline:**
"Machine Learning, which supports traditional AI, is roughly divided into four categories based on the method of learning."

**The 4 Categories:**

1.  **Supervised Learning** (Teacher-based)
    *   **Definition:** Learning using labeled data. Input and the "correct answer" (label) are provided. The AI learns to produce an output close to the correct answer.
2.  **Unsupervised Learning** (No Teacher)
    *   **Definition:** Learning using data *without* labels. The AI identifies patterns and analyzes structures in the data without being told what is "right."
3.  **Reinforcement Learning**
    *   **Definition:** The AI repeats trials within a given environment and learns to maximize rewards. It is not given explicit correct answers but improves behavior based on the outcome (reward/punishment).
4.  **Deep Learning**
    *   **Definition:** The AI automatically learns features from massive amounts of data, enabling high-precision prediction and generation.

---

### **Slide 23: Machine Learning Details (1/2)**

**Headline:**
**Machine Learning (1/2)**

**Sub-headline:**
"A method to predict future data or automatically discover hidden structures in data using sample data. Here are the 3 representative methods."

**Column 1: Supervised Learning**
*   **Diagram:** Labeled Data $\rightarrow$ Regression or Classification $\rightarrow$ Output.
*   **How it works:** You give the AI examples (e.g., "This email is Spam," "This email is Safe").
*   **Output Examples:** Sales Forecasting, Fraud Detection, Failure Diagnosis, Spam Mail Judgment.

**Column 2: Unsupervised Learning**
*   **Diagram:** Raw Data $\rightarrow$ Clustering or Dimensionality Reduction $\rightarrow$ Output.
*   **How it works:** You throw data at the AI (e.g., customer purchase history) and ask it to find groups.
*   **Output Examples:** Segment Classification (e.g., grouping customers by behavior).

**Column 3: Reinforcement Learning**
*   **Diagram:** Data $\rightarrow$ Optimization (Loop of Evaluation/Reward) $\rightarrow$ Output.
*   **How it works:** Trial and error.
*   **Output Examples:** Elevator Control, Advertising Optimization (finding the best ad placement through testing).

---

### **Slide 24: Machine Learning Details (2/2) - Deep Learning**

**Headline:**
**Machine Learning (2/2)**

**Definition:**
"Deep Learning is a machine learning method that learns trends and rules of given data using a **multi-layered structure** (Neural Network). Unlike normal machine learning, it automatically analyzes features from massive amounts of data, understands what it should learn, and performs trial and error."

**The Neural Network Diagram:**
*   **Input Layer (Left):** An image of a ship is fed into the system.
*   **Hidden/Middle Layers (Center):** "Analysis of Data." The web of nodes represents the "Deep" part. It processes the image through many layers of abstraction to identify shapes, edges, and features.
*   **Output Layer (Right):** The system outputs a probability score.
    *   **Container Ship: 95%** (The AI is confident)
    *   **Cruise Ship: 5%**
 

---

### **Slide 25: Limitations of Traditional AI**

**Headline:**
**Limitations of Traditional AI**

**Summary Text:**
"Traditional AI focused on discrimination and classification. It was premised on having a set of massive teacher data (training data) and labels. Therefore, in addition to high implementation costs, it was weak in non-routine tasks, generation, and dialogue, making the scenarios where it could be used in practice limited."

**The Three Bottlenecks:**

1.  **Output is Centered on Discrimination/Classification Uses:**
    *   It could only say "Yes/No" or "This is a Cat/Dog." It couldn't create.
2.  **Requires a Set of Massive Data and Labels:**
    *   You needed thousands of photos manually labeled "Cat" to teach it. This is labor-intensive.
3.  **High Cost & Limited Deployment:**
    *   "For non-standard tasks or utilization in generation/dialogue, the learning and maintenance costs for each specific use case are high, making deployment to the field limited."

---

### **Slide 26: Section Title**

**Text:**
**Generative AI**

*Context: A simple title slide introducing the new topic.*

---

### **Slide 27: What is Generative AI?**

**Headline:**
**What is Generative AI?**

**Definition:**
"Generative AI is a collective term for AI capable of generating sentences, images, etc. Representative examples include ChatGPT and Sora."

**Visual Examples (The "Big Players"):**

1.  **ChatGPT (Bottom Left):** The standard for text generation (OpenAI).
2.  **Sora (Top Middle):** OpenAI's text-to-video model. The tagline "From words to worlds" highlights its ability to create realistic video from text.
3.  **Gen-4.5 (Top Right):** A video generation model by Runway, showing a parrot and cactus, demonstrating high-fidelity creative video.
4.  **Gemini (Bottom Middle):** Google's multimodal AI, shown integrated into Google Workspace (Docs/Gmail) for business writing.
5.  **Co-pilot (Bottom Right):** Microsoft's AI assistant, shown with a simple prompt interface ("What would you like to create today?").

---

### **Slide 28: Generative AI Market Size / Forecast**

**Headline:**
**Generative AI Market Size / Forecast**

**Summary Text:**
"The global Generative AI market size was 10.14 billion USD in 2022 and is predicted to reach 109.37 billion USD by 2030."

**Chart 1 (Left): Forecast Transition of Global GenAI Market Size**
*   **The Trend:** Massive exponential growth.
*   **Key Stat:** **CAGR (Compound Annual Growth Rate) of 34.6%** between 2022 and 2030.
*   **Breakdown:**
    *   **Light Orange (Software):** The larger portion. This represents the AI models and applications themselves (like paying for ChatGPT Enterprise).
    *   **Dark Orange (Services):** The smaller but growing portion. This represents consulting, implementation, and maintenance services.

**Chart 2 (Right): Global GenAI Market Composition (2022)**
*   **USA (31.3%):** Clearly the dominant leader in the market.
*   **Canada (8.9%)** & **China (7.6%):** The next tier.
*   **Japan (7.0%):** Japan holds a significant but smaller portion of the global pie.
*   **India (6.4%)** & **Germany (4.3%):** Emerging and European hubs.

---

### **Slide 29: Environmental Changes due to Generative AI**

**Headline:**
**Environmental Changes due to Generative AI**

**Introductory Text:**
"Companies need to predict the rapid changes brought about by Generative AI, and without being bound by existing frameworks, consider management transformation starting from now."

**The Future Timeline (2030 - 2050):**
This slide maps out a futuristic prediction of how AI will alter society and work.

*   **2030: Change in Labor Environment**
    *   "Working hours decrease, and the areas where individuals can be active expand."
*   **2035: Change in Talent Demand**
    *   "Demand for talent specializing in advanced utilization of Generative AI increases, while demand for general white-collar work decreases."
    *   *Note: This suggests a shift from 'doing the work' to 'managing the AI that does the work'.*
*   **2040: Scientific Development**
    *   "By utilizing Generative AI, the creation of new material properties and robot development progresses rapidly."
*   **2045: Intelligent Robots**
    *   "Robots possessing intelligence are born." (Often referred to as the Singularity).
*   **2050: Liberation from Labor**
    *   "Intellectual/creative work and physical labor are replaced by robots; about half of humans will no longer need to work."
    *   "Almost all labor is performed by AI; humans will be able to concentrate on childcare, nursing care, etc."

**Summary:** The lecture presents a vision where AI evolves from a business tool (2030) to a scientific accelerator (2040) and finally to a societal replacement for labor (2050).


---

### **Slide 30: Business Replacement by Generative AI**

**Headline:**
**Business Replacement by Generative AI**

**Summary Text:**
"Generative AI is expected to replace many job types, with economic effects predicted to boost global GDP by 7%. The majority opinion is that many business functions, from materials/purchasing to accounting/finance, will be replaced by Generative AI, and this trend is assumed to progress."

#### **Chart 1 (Left): "Survey Results on Productivity in Each Country"**
*   **Source:** Goldman Sachs.
*   **Y-Axis:** Productivity increase (percentage points).
*   **Data:** Shows the estimated annual productivity growth over a 10-year adoption horizon for various countries.
*   **Key Highlight:** The **Global** average (highlighted box) shows a **1.5pp** increase.
*   **Bottom Box:** "Global productivity improves by **1.5pp**, and GDP increases by **7%**."

#### **Chart 2 (Right): "Survey Results on Business Replacement"**
*   **Question:** "How much of your work do you think will be replaced by Generative AI?"
*   **Comparison:** Spring 2024 vs. Spring 2025.
*   **The Data:**
    *   **Fully Replaced (100%):** 3% $\rightarrow$ 5%
    *   **Mostly Replaced (60-80%):** 19% $\rightarrow$ 22%
    *   **Half Replaced (50%):** 26% $\rightarrow$ 31%
    *   **Somewhat Replaced (20-40%):** 44% $\rightarrow$ 35%
    *   **Not Replaced (0%):** **Only 4%**.
*   **Bottom Box:** "The view that 'Business will not be replaced' is a minority."
*   **Insight:** Over 90% of respondents believe at least *some* portion of their job will be automated.

---

### **Slide 31: Types of Generative AI**

**Headline:**
**Types of Generative AI**

**Sub-text:**
"Generative AI can be broadly classified based on what it generates: text, program code, images/video, voice/music, etc."

**Categories (Recap of previous slides):**
This slide serves as a transition into the technical definitions.
1.  **Text:** Marketing copy, emails, summaries.
2.  **Program Code:** Documentation, SQL generation.
3.  **Image / Video:** Web design, medical imaging.
4.  **Voice / Music:** Presentation voice, music composition.
5.  **Other:** 3D avatars, startup support.

---

### **Slide 32: What is an LLM?**

**Headline:**
**What is an LLM?**

**Definition:**
"LLM (Large Language Model) is a natural language processing model that understands text and generates appropriate sentences. It exists within the larger category of Generative AI."

**The Diagram Flow:**

1.  **Input (Left):**
    *   **Text Data:** The raw material.
2.  **Learning Phase:**
    *   "Learns patterns and context from massive datasets."
3.  **The Core (Center):**
    *   **LLM (Orange Oval):** The trained model.
    *   **Prompt:** Information/instructions provided by a human (User).
4.  **Output Phase:**
    *   "Responds based on pre-learned data."
5.  **Result (Right):**
    *   **Language Generation:**
        *   Code generation
        *   Text summarization
        *   Q&A
        *   Translation

---

### **Slide 33: What is a VLM?**

**Headline:**
**What is a VLM?**

**Definition:**
"VLM (Vision-Language Model) is a model that processes visual information (images/videos) and language information (text/audio) simultaneously and integrally. It can perform complex processing such as explaining image content or generating images based on text instructions."

**The Diagram Flow:**

1.  **Input (Left):**
    *   **Pairs of Text + Video/Image:** Unlike LLMs, VLMs learn the *relationship* between pixels and words.
2.  **Learning Phase:**
    *   "Learns the relationships between different types of data (heterogeneous data) from massive datasets."
3.  **The Core (Center):**
    *   **VLM (Orange Oval):** The multimodal model.
    *   **Prompt:** "Information from human."
    *   *Specific Example:* The user uploads a photo of a ship and asks, "Explain this image in under 15 characters."
4.  **Output Phase:**
    *   "Responds based on pre-learned data."
5.  **Result (Right):**
    *   **Language Generation:** "A container ship is docked at the port."
    *   *Capabilities:* Visual Q&A, Image Captioning, Medical Image Analysis.

---

### **Slide 34: What is the Transformer (The Foundation of GenAI)?**

**Headline:**
**What is the "Transformer" that became the foundation of Generative AI?**

**Definition:**
"The Transformer makes it easier to proceed with learning in parallel compared to traditional methods. Because it can learn massive amounts of data faster, Generative AI models can now train on larger scales, significantly improving output accuracy and expressiveness."

#### **Left Diagram: Transformer Mechanism (Architecture)**
*   This shows the famous Neural Network architecture diagram.
*   **Encoder (Left block):** Converts input data into a format the machine can process.
*   **Decoder (Right block):** Processes data received from the encoder to generate output.
*   **Key Component:** **Attention** (Highlighted in boxes). This is the secret sauce that allows the model to weigh the importance of different words.

#### **Right Diagram: Mechanism of "Attention"**
*   **Concept:** "Grasps the context as a whole, processes in parallel, and finds the important parts (Attention)."
*   **Visual Example:**
    *   Sentence: "I learn AI management" (Watashi wa AI keiei wo manabi masu).
    *   **Attention Score:** The model assigns scores (e.g., 0.6, 0.03) to understand which words relate to each other. For example, "I" relates strongly to "learn."
*   **Orange Box:**
    *   **"Long-term Memory"** (It doesn't forget the beginning of the sentence by the time it reaches the end).
    *   **"Fast Learning Speed"** (Due to parallel processing).

#### **Bottom Box: Traditional Method (Comparison)**
*   **Method:** Processed sequentially based on rules or labels.
*   **Flow:** "I" $\rightarrow$ "wa" $\rightarrow$ "AI" $\rightarrow$ "management"...
*   **Flaw:** "Forgets old memories during the middle of processing."
*   **Result:** "Slow Learning Speed."

**Key Takeaway:** The Transformer revolutionized AI because it stopped reading like a human (one word at a time, left to right) and started reading like a computer (absorbing the whole sentence at once and mathematically calculating the relationships between words instantly).


---

### **Slide 35: What is a Reasoning Model?**

**Headline:**
**What is a "Reasoning Model" that improved AI's inference ability?**

**Definition:**
"A Reasoning Model is a Generative AI model designed to perform multi-step thinking before generating an answer. By repeating thoughts on its own, the inference ability is significantly strengthened compared to traditional models."

*Note: This refers to models like OpenAI's **o1** series, which "think" before they speak.*

**Left Diagram: Mechanism of Reasoning Model Learning**
*   **Concept:** "In addition to a general pre-trained model, it performs additional learning using problems and **CoT (Chain of Thought)** prompts that solve them. By repeating multiple inferences, it approaches the answer step-by-step."
*   **The Equation:**
    *   [General Pre-trained Model] **+** [Reasoning Model Training]
    *   **Process:**
        1.  **Supervised Learning:** Learning from "Problem + Thinking Method to solve it (CoT Prompt)."
        2.  **Reinforcement Learning:** The model generates its own CoT prompts $\rightarrow$ Evaluation/Reward $\rightarrow$ Improvement.
        3.  **Result:** "Generates CoT prompts by itself."

**Right Box: CoT (Chain of Thought) Shift**
*   **Old Way:** Humans gave the instructions.
    *   *User:* "Think step by step!" $\rightarrow$ *AI:* (Tries to follow).
*   **New Way (Reasoning Model):** Reinforcement learning allows the AI to do this autonomously.
    *   *Flow:* Problem $\rightarrow$ Inference 1 $\rightarrow$ Inference 2 $\rightarrow$ ... $\rightarrow$ Answer.
    *   *Key Insight:* **"The higher the number of inference steps, the higher the accuracy."**

---

### **Slide 36: How to Improve Generative AI Accuracy (The 4 Approaches)**

**Headline:**
**How to heighten the accuracy of Generative AI**

**Sub-text:**
"You don't just use the general foundation model as is; you can improve accuracy through various approaches."

**The Table Analysis (Important Framework):**

This table compares the four standard methods for customizing AI.

| Approach Method | Definition | Required Data | Quality (Accuracy) | Implementation Difficulty | Update Cost (Freq / Per time) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **1. Prompt Engineering** | Designing and optimizing instructions/commands to get the desired output from the LLM. | **None** (Uses pre-trained model). | **Low** | **Low** | Low / Low |
| **2. RAG** (Retrieval-Augmented Generation) | Searching external databases and having the AI generate answers based on the latest info. | Internal regulations, product info, competitor info, manuals, etc. | **Medium** | **Medium** | **High Freq** / Low Cost |
| **3. Fine-tuning** | Re-training part or all of a pre-trained model using a specific dataset. | Thousands of domain-specific data points, instruction samples. | **High** | **High** | Low Freq / **High Cost** |
| **4. Pre-training** | Training a massive dataset to acquire language understanding capabilities. Specialized for niche industries where no correct labels exist. | Massive datasets (Billions to Trillions of tokens). | **Highest** | **Highest** | Low Freq / **Highest Cost** |

**Visual Analysis (Orange Bars):**
*   **Prompt Engineering** is easiest but has a quality ceiling.
*   **Pre-training** yields the best quality for niche tasks but is prohibitively expensive for most companies.
*   **RAG** is unique because the "Update Frequency" is high (orange diamond), meaning it's best for data that changes every day (like news or stock prices).

---

### **Slide 37: (1) Prompt Engineering**

**Headline:**
**(1) Prompt Engineering**

**Definition:**
"To obtain more desirable generation results, the user devises the question or instruction text (Prompt) input into the AI model."

**Comparison:**

*   **Bad Example (Top):**
    *   *Prompt:* "Make a template for inquiry response."
    *   *Result:* "Subject: Thank you for your inquiry. Body: We have received your inquiry regarding... We will contact you once we know the result."
    *   *Critique:* Too generic.
*   **Good Example (Bottom):**
    *   *Prompt:* "Create a primary response template for a **SaaS login failure**. Reply within **1 hour**, communicate resolution prospect within **24 hours**. Tone should be **polite and concise**."
    *   *Result:* "Subject: Notice of Primary Response regarding Login Failure (Case: [Number]). Body: Dear [Customer Name], Thank you for contacting us regarding the inability to login. We will provide a primary response within 1 hour... etc."
    *   *Verdict:* **"A more desirable result matching the conditions."**

---

### **Slide 38: (2) RAG Mechanism**

**Headline:**
**(2) Mechanism of RAG (Retrieval-Augmented Generation)**

**Definition:**
"By passing information necessary to answer the question *together* with the question, RAG makes it possible to answer things traditional LLMs found difficult, such as **'Answer regarding unique internal regulations'** or **'Answer regarding the latest news articles'**."

**The Process Diagram:**
1.  **Retrieval:** The system searches for necessary information (e.g., from a database of graphs/docs).
2.  **Augment:** The system combines the **Question** + **Search Results**.
3.  **Generation:** The LLM generates an answer based on that combined data.

**Case Study: Manufacturing Industry (Right Box)**
*   **Challenge:** In manufacturing, there are CAD drawings, handwritten notes on-site, and images. Standard text-based LLMs couldn't handle these.
*   **Solution:** Using RAG allows the embedding of drawings and images in addition to text documents. This enables the output of internal knowledge searches and document files that the LLM has "understood."

---

### **Slide 39: (3) Fine-tuning**

**Headline:**
**(3) Fine-tuning**

**Definition:**
"By performing additional training using a dataset for a specific task, the Generative AI model itself can be specialized for that specific task."

**Left Diagram: Overview**
*   **Input:** General LLM (trained on massive general data).
*   **Process:** **Additional Learning** using "Company-specific data" or "Task-specific datasets."
*   **Output:** Specialized LLM (e.g., an AI that speaks exactly like your brand's customer service rep).

**Right Diagram: Types of Fine-tuning**
1.  **Supervised Fine-tuning:**
    *   Creates a model aimed at a specific task where a "correct answer" exists.
    *   *Analogy:* Studying with a textbook where the answers are in the back.
2.  **Reinforcement Learning Fine-tuning:**
    *   Used for ambiguous content like "conversational tone."
    *   *Mechanism:* You set a **"Rule Book"** (Reward Model).
    *   *Cycle:* The LLM outputs text $\rightarrow$ The Reward Model checks it against the Rule Book $\rightarrow$ Gives a reward based on the output $\rightarrow$ The LLM learns to maximize the reward.
  

---

### **Slide 40: Challenges of Generative AI**

**Headline:**
**Challenges of Generative AI**

**Summary Text:**
"Because Generative AI was limited to responding to single-shot instructions, and the granularity of instructions determined output quality, there was a challenge in that **it was difficult to perform a series of tasks spanning multiple processes or autonomous task execution**."

**Left Panel: The Continuity Challenge**
*   **Headline:** "Even if it can do single tasks, it is difficult to entrust it with a connected series of business processes."
*   **Comparison:**
    *   **Task Unit (Orange):** Capable of responding to single instructions.
    *   **Business Unit (Grey):** Difficult to handle tasks that bundle a series of operations.

**Right Panel: The Autonomy Challenge**
*   **Headline:** "Because continuous guidance is necessary to achieve goals, autonomous task execution is difficult."
*   **Comparison:**
    *   **Continuous Guidance (Orange):** The quality of the output depends on the granularity of the user's instructions (Prompt Engineering).
    *   **Autonomous Execution (Grey):** It has not yet reached the point of autonomous task execution (where the AI figures out the steps itself).

---

### **Slide 41: Definition of AI Agent**

**Headline:**
**Definition of AI Agent**

**Definition Box:**
"While there is no unified definition, it refers to a program that **interprets user instructions, autonomously plans what should be done, and executes tasks to achieve the goal**."

**The Architecture Diagram:**

1.  **User (Left):**
    *   Gives a high-level instruction: *"Prepare the necessary materials for the meeting the day after tomorrow."*
2.  **The AI Agent Core (Center Box):** The "Brain" acts in a loop.
    *   **Interpret:** Interprets user instructions.
    *   **Plan:** Based on the interpretation, plans tasks to achieve the goal.
    *   **Execute:** Uses external data and tools to execute tasks.
    *   **Adapt:** Flexibly adapts and corrects the plan based on execution results.
    *   **Learn:** Learns from execution results to optimize future actions.
3.  **External Connections (Top):**
    *   **Tools:** The Agent can "Operate" tools and receive "Responses" (e.g., Calendar, Email, Slack).
    *   **Data Sources:** The Agent can "Search" and "Read" data (e.g., Company Wiki, CRM).
4.  **Output (Right):**
    *   *Result:* "I created materials for tomorrow's 'XX MTG'. I also created a folder with related materials and stored them. Here is the link."
    *   *Deliverables:* Research results, Analysis reports, Code, etc.

---

### **Slide 42: What Can AI Agents Do?**

**Headline:**
**What Can AI Agents Do?**

**Introductory Text:**
"Unlike Chatbots, RPA, or standard Generative AI, AI Agents execute tasks flexibly in response to inputs, enabling the execution of various business operations."

**The Two Categories of Value:**

**1. Substitution of Work (Automation)**
*   **Concept:** "The AI Agent acts as a substitute, autonomously planning and acting based on goals."
*   **Concrete Examples:**
    *   **Autonomous Execution of Sales Process:** Not just simple analysis, but analyzing CRM data, estimating potential customers, considering appropriate follow-ups, and executing them.
    *   **Autonomous Execution of Recruitment Process:** Drafting job descriptions, finding passive candidates, contacting candidates, and autonomously judging/recommending the best talent for open positions.

**2. Sophistication of Work (Augmentation)**
*   **Concept:** "Processing massive amounts of data and executing work while eliminating emotion—leveraging the unique characteristics of AI Agents to upgrade business operations."
*   **Concrete Examples:**
    *   **Price Negotiation:** The Agent finds the optimal negotiation point from massive past transaction data and negotiates with suppliers to procure at lower prices.
    *   **Sophistication of R&D:** Analyzing massive past data to perform cross-sectional examinations humans can't think of, proposing appropriate research approaches.
    *   **Corporate Analysis:** Analyzing and judging companies from diverse perspectives humans can't fully cover (credibility, past transactions, macroeconomics, finance).

---

### **Slide 43: Sophistication of Work Utilizing AI Agents (Multi-Agent Example)**

**Headline:**
**Sophistication of Work Utilizing AI Agents**

**Sub-text:**
"Complex systems can be created, such as 'Creating a business plan utilizing AI Agents based on a theme given by the user.'"

**The Diagram (A Multi-Agent Team Structure):**

*   **User (Left):** "Please create a business plan utilizing AI Agents for the healthcare industry."
*   **The Commander Agent (Center):** This is the Project Manager. It receives the instruction and manages the sub-agents.

**The Sub-Agents (The Team):**
1.  **Feasibility Evaluation Agent (Top):**
    *   Evaluates if the proposed ideas are realistic.
2.  **Business Plan Generation Agent (Middle):**
    *   Uses "Case Study Acquisition Tools" to collect examples.
    *   Drafts the actual plan.
3.  **Market Analysis Agent (Bottom):**
    *   Uses "Information Search Tools" to read internal market research reports.
    *   Analyzes the market data.

**Key Takeaway:** The user doesn't have to prompt the Market Agent, then the Plan Agent, then the Evaluation Agent. The User talks to the Commander, and the Commander coordinates the specialist bots to deliver a final result.

---

### **Slide 44: Forms of AI Agents**

**Headline:**
**Forms of AI Agents**

**Summary Text:**
"AI Agents are classified into two forms: **Single Agent** and **Multi-Agent**. Multi-Agents are further classified into Hierarchical, Horizontal Cooperative, and Horizontal Competitive."

**The Matrix Analysis:**

| Agent Form | Single Agent | Multi-Agent: Hierarchical | Multi-Agent: Horizontal (Cooperative) | Multi-Agent: Horizontal (Competitive) |
| :--- | :--- | :--- | :--- | :--- |
| **Structure** | **User $\leftrightarrow$ Agent** | **Commander $\rightarrow$ Subordinates** | **Agent $\leftrightarrow$ Agent (Cooperation)** | **Agent $\leftrightarrow$ Agent (Competition)** |
| **Definition** | Operates as a single entity. | Operates with multiple agents. A **Commander Agent** controls subordinate agents. | Operates with multiple agents. Agents **coordinate** with each other. | Operates with multiple agents. Agents **compete** with each other. |
| **Use Case** | Used where complex decision-making is not required, such as drafting email replies. | Used where centralized management is necessary, such as corporate management/strategy. | Used where joint work or resource sharing is necessary, such as system development (Dev + Ops). | Used where creative results or complex stakeholder intentions are involved (e.g., R&D, Debate simulations). |
| **Image** | 1-on-1 interaction. | Top-down command structure. | Side-by-side collaboration arrows. | Arrows clashing (Conflict/Debate). |

**Insight:** The "Competitive" model is particularly interesting. It suggests setting up two AIs to debate each other (e.g., one proposing a strategy, the other acting as a critical investor) to refine an idea before presenting it to a human.

Based on the additional slides provided, here is the detailed translation and explanation. The presentation dives deeper into the architecture of **AI Agents** and then transitions into the next frontier: **Physical AI** (Robotics).

---

### **Slide 45: Elements Supporting AI Agent Autonomy**

**Headline:**
**Elements Supporting AI Agent Autonomy**

**Introductory Text:**
"To sophisticated thinking and behavior in AI Agents, multiple core elements such as Profile, Memory, Planning, and Action work in coordination."

**The Four Core Elements:**

1.  **Profile**
    *   **Definition:** "Demonstrating consistent behavior according to a role."
    *   **Detail:** Defines the role, purpose, and attributes that determine behavior guidelines and expertise.
    *   *Examples:* Coordinator role, Information Search role, Analyst role, Reporting role.

2.  **Memory**
    *   **Definition:** "Accumulating and sharing information and experience over the long term."
    *   **Detail:** An element that saves past info and experiences to utilize for future judgments. You can define what should be kept short-term vs. long-term.

3.  **Planning**
    *   **Definition:** "Autonomously executing complex workflows."
    *   **Detail:** The element that breaks a task into subtasks and builds a plan to achieve the goal.

4.  **Action**
    *   **Definition:** "Executing complex tasks according to the role."
    *   **Detail:** Executing tasks based on the role and acquiring results. Interacting with the external environment (Tools).
    *   *Examples:* Web search function, File search function, Coding function, Reporting function.

---

### **Slide 46: Sophistication of AI Agents (RAG & Connectivity)**

**Headline:**
**Sophistication of AI Agents**

**Introductory Text:**
"Connecting AI Agents to external resources is becoming easier through Agent-based RAG, MCP (Model Context Protocol), and A2A (Agent to Agent), expanding the scope of use cases."

**Diagram Analysis:**

**Left Side: Internal Resources (Agent-based RAG)**
*   **Concept:** Instead of just searching once, the Agent **self-corrects**.
*   **The Loop:**
    1.  User asks a question.
    2.  Agent drafts an answer.
    3.  **Self-Check:** "Is more information needed?"
    4.  If Yes $\rightarrow$ **Resource Search** (Search internal knowledge/DB).
    5.  Update the Prompt with new info.
    6.  **Regenerate Answer.**
    7.  **Final Check:** "Is the answer appropriate?"
    8.  If No $\rightarrow$ Loop back. If Yes $\rightarrow$ Output to user.

**Right Side: External Resources (Protocols)**
*   **MCP (Model Context Protocol):**
    *   A common protocol for connecting to external systems and databases. It standardizes how the AI "plugs in" to data tools.
*   **A2A (Agent to Agent):**
    *   **Definition:** A common protocol for AI Agents that function individually to collaborate across companies or systems.
    *   *Visual:* An Agent talking to another Agent directly, bypassing humans.

---

### **Slide 47: Introduction Patterns of AI Agents**

**Headline:**
**Introduction Patterns of AI Agents**

**Introductory Text:**
"Services utilizing AI Agents, as well as platforms, frameworks, and libraries for introducing them, are being established, lowering the hurdles for adoption."

**The Four Categories (From Easiest to Most Complex):**

1.  **Services utilizing AI Agents (SaaS)**
    *   **Description:** Services aimed at substituting or upgrading work using AI Agents.
    *   **Status:** Services are emerging that handle entire business processes (like sales), going beyond simple analysis.
2.  **Integrated Platform**
    *   **Description:** Providing the implementation foundation. Cloud-based environments to build/run agents.
    *   **Status:** "Cloud Agent" services are available, removing the need for complex environment setup.
3.  **Framework**
    *   **Description:** Providing the skeletal structure. You define the agent along the framework to execute it.
    *   **Status:** Environments enabling easy construction and execution management (like LangChain) are available.
4.  **Library**
    *   **Description:** Providing parts for implementation. Useful components for building agents from scratch.
    *   **Status:** Libraries are enriched, and the environment for building from scratch is well-developed.

---

### **Slide 48: Expanding the Field of Activity (Cyber to Physical)**

**Headline:**
**Expanding the Field of AI Agent Activity**

**Introductory Text:**
"AI has automated digital tasks ranging from simple document search to complex workflows involving agent collaboration. The next step is jumping out of the digital and into the **Physical World**, automating the physical world."

**Comparison Diagram:**

**Top: Cyber Space (Current)**
*   **Input:** Text, Video/Image.
*   **Process:** **LLM** (Large Language Model) / **VLM** (Vision Language Model).
*   **Output:** Text, Video/Image.
*   *Domain:* Digital Documents, Chat, Virtual Assistants.

**Bottom: Physical Space (Future/Emerging)**
*   **Input:** Sensor Data, Tactile (Touch) Data, etc.
*   **Process:** **VLA** (Vision-Language-Action).
    *   *Note:* This is a model trained not just on words, but on *physics and movement*.
*   **Output:** **Motion** (Torque drive, movement).
*   *Domain:* Robotics, Autonomous Vehicles, Smart Factories.

---

### **Slide 49: Section Title**

**Text:**
**Physical AI / Embodied AI**

*Context: The presentation formally moves to the topic of Robotics powered by AI.*

---

### **Slide 50: Robot Control (Manipulation vs. Mobility)**

**Headline:**
**Robot Control**

**Introductory Text:**
"Robot control is broadly divided into two types: **Manipulation** (Hand movement) and **Mobility** (Leg movement). In environments where both manipulation and mobility have diversity, more complex control becomes necessary."

**The Matrix (Challenges in Robotics):**

*   **Y-Axis: Manipulation** (Diversity of Work/Task).
    *   *Low:* Simple repetitive tasks.
    *   *High:* Handling diverse objects (soft, hard, heavy, fragile).
*   **X-Axis: Mobility** (Diversity of Environment).
    *   *Low:* Fixed environment (e.g., a bolted-down factory arm).
    *   *High:* Different environments (e.g., moving through a messy home or changing warehouse).

**The Quadrants:**
*   **Bottom Left (Current Status):** "Repeating the exact same work in a fixed environment." (Traditional Industrial Robots).
*   **Top Right (The Goal - Orange Arrow):** "Handling diverse objects in different environments."
    *   **Requirement:** "More complex control of Hands and Legs is required."

**Key Insight:** Physical AI aims to move robots from the bottom-left (dumb factory arms) to the top-right (intelligent general-purpose robots).

Based on the final set of slides provided, here is the detailed translation and explanation. These slides conclude the presentation by contrasting **Traditional Robotics** with the new **Physical AI**, mapping the evolutionary history of AI, and providing logistical details for the course.

---

### **Slide 52: Robot Control Utilizing Traditional AI**

**Headline:**
**Robot Control Utilizing Traditional AI**

**Introductory Text:**
"Robots understand their surrounding environment and objects, planning and executing appropriate paths and movements. Robot control using recent AI (Traditional) required **learning rules in advance**, specializing in tasks with **fixed objects** in **fixed environments**."

**Left Panel: Manipulation (Hand Movement)**
*   **Definition:** A series of operations: "Grab, Move, Place, Use if necessary."
*   **The Diagram:**
    *   **Condition:** Fixed Object (The robot always expects the same part in the same spot).
    *   **Process:**
        1.  **Learn Rules:** (e.g., "If sensor A triggers, move arm to coordinate X,Y,Z").
        2.  **Input:** Camera/Sensor data.
        3.  **Plan Trajectory:** The arm moves on a pre-calculated rail.
    *   **Questions it answers:** "What type of object? Where should I hold it to be stable? What is the appropriate acceleration?"

**Right Panel: Mobility (Leg Movement)**
*   **Definition:** "The ability to change position and move safely and reliably within an environment."
*   **The Diagram:**
    *   **Condition:** Fixed Environment (Obstacles are static/bolted down).
    *   **Process:**
        1.  **Learn Rules:** (e.g., "Follow the magnetic tape on the floor").
        2.  **Input:** Camera/Sensor data.
        3.  **Plan Path.**

**Bottom Line (Orange Box):**
"Specialized for tasks with **fixed objects** in **fixed environments** by making them learn rules in advance."

---

### **Slide 53: Robot Control Utilizing Physical AI**

**Headline:**
**Robot Control Utilizing Physical AI**

**Introductory Text:**
"Foundations are being researched and developed to allow robots to autonomously judge appropriate trajectories and paths in response to **diverse objects** and **diverse environments**. The collective term for these foundations is **Physical AI**."

**Left Panel: Manipulation**
*   **Condition:** **Diverse Objects** (Square, Triangle, Circle—messy pile).
*   **Process:**
    *   Input: Visual/Tactile info.
    *   **The Brain:** **Physical AI** (Instead of a Rulebook). It "figures out" how to grab a weirdly shaped object it has never seen before.

**Right Panel: Mobility**
*   **Condition:** **Diverse Environments** (Moving obstacles, people walking around).
*   **Process:**
    *   **The Brain:** **Physical AI**.
    *   **Questions it answers autonomously:**
        *   "Where am I right now?"
        *   "Where is the target?"
        *   "Which way is that person likely to move?" (Prediction).

**Bottom Line (Orange Box):**
"The robot perceives the surrounding environment by itself and judges the appropriate trajectory/path."

---

### **Slide 54: Examples of Physical AI**

**Headline:**
**Examples of Physical AI**

**Introductory Text:**
"In Mobility, the generalization of navigation foundations is progressing. Meanwhile, the immediate trend is the expansion of Manipulation capabilities via **VLA**, and research is progressing on understanding situations/instructions via vision and language, and dropping them into concrete actions like grasping."

**Left Box: Manipulation Focus (VLA)**
*   **VLA (Vision-Language-Action):**
    *   **Definition:** An AI model that connects the understanding of **Vision** and **Language** to concrete **Action**.
    *   **The Flow Diagram:**
        1.  **Vision:** "The mandarin orange is on the table." (Robot sees it).
        2.  **Language:** User says, "Pick up the orange."
        3.  **Action:** The Robot physically grabs and lifts the orange.
    *   *Key:* The robot understands the *semantic concept* of an "orange" and "pick up," allowing it to interact with objects it hasn't been explicitly programmed for.

**Right Box: Mobility Focus (GNM)**
*   **GNM (General Navigation Model):**
    *   **Definition:** A general-purpose navigation foundation model that can reach a destination based on visual information even in **unknown situations**.
    *   **The Flow Diagram:**
        *   **Training:** Learned from data (drones, walking robots, etc.).
        *   **Execution:** Robot A and Robot B (different hardware) use the same GNM brain.
        *   **Result:** Capable of "Zero-shot" navigation (handling a new environment for the first time without prior mapping).

---

### **Slide 55: Expectations for Physical AI (The Future Timeline)**

**Headline:**
**Expectations for Physical AI**

**Timeline S-Curve Analysis:**
This slide shows three waves of evolution merging into one.

1.  **Phase 1: Specific Use (Left)**
    *   *Cyber:* IT Systems.
    *   *Physical:* Stable, high-precision repetitive control.
    *   *State:* **Mature Hardware** / **Limit of Existing Software**.

2.  **Phase 2: Flexible Recognition & Judgment (Middle)**
    *   *Cyber:* Evolution of AI/Digital (The "Big Brain").
    *   *Physical:* Traditional Control + Linkage with Cyber.
    *   *State:* **Human-level flexible recognition.** Handling variation in objects and environments. Multi-skilled work.
    *   *Limit:* **Limit of Existing Hardware**.

3.  **Phase 3: Expert-level Movement (Right - The Future)**
    *   *Goal:* **Unified Evolution of Hardware/Software.**
    *   *Capabilities:*
        *   **Movements on par with a skilled trainee.**
        *   Real-time integrated execution of recognition, judgment, and motion.
        *   Work utilizing integrated "five senses" (touch, sight, sound, etc.).

---

### **Slide 56: AI Up To Now (Evolution Summary)**

**Headline:**
**AI Up To Now**

**Summary Text:**
"AI has expanded its application area in stages along with technological evolution: from Traditional AI centered on discrimination/classification, to the emergence of Generative AI due to dramatic improvements in context understanding and inference, to AI Agents, and finally reaching Physical AI."

**The Evolution Graph (Time vs. Tech Evolution):**

*   **Step 1: Traditional AI (Bottom Grey):**
    *   Discrimination, Classification.
*   **Step 2: Generative AI (Middle Grey):**
    *   **LLM** (Large Language Models).
    *   **Transformer** architecture.
    *   **VLM** (Vision Language Models).
*   **Step 3: AI Agents (Light Grey):**
    *   **Reasoning Models** (Thinking before acting).
    *   **MCP / A2A** (Connecting tools and agents).
*   **Step 4: Physical AI / Embodied AI (Top Orange):**
    *   **VLA** (Vision-Language-Action).
    *   *Message:* We are currently entering the era where AI leaves the screen and enters the physical world.

---

### **Slide 57: Information on the 1st Lecture**

**Headline:**
**Information on the 1st Lecture**

**Details:**

1.  **Lecture Date & Time:**
    *   **January 21st (Wednesday) 19:00.**
    *   Please participate in real-time if convenient. (Archive distribution is available).

2.  **How to Participate:**
    *   The **Zoom URL** for attendance is included in the email sent to you.
    *   *(Warning: Sharing with non-students or posting on SNS is strictly prohibited).*

3.  **Communication Tool:**
    *   **Use of Slack.**
    *   Important announcements will be made via Slack, so please join.
    *   Please use it as a place for interaction with people from various industries interested in AI.

**Bottom Left:** pwc.com

Based on the final image provided, here is the translation and explanation of the closing slide.

---

### **Slide 58: Introduction to PwC Services**

**Headline:**
**Introduction to PwC Services**

**Body Text:**
"**Making management with AI the standard** – From strategy design to business planning, PoC (Proof of Concept), operation, and internal education, we provide information on how to proceed tailored to the specific stage of your company."

**Call to Action (Center):**
"**Contact us here**"
*(The QR code is provided for viewers to scan and access the inquiry page).*

**Explanation:**
This is the final "sales pitch" slide. After providing a comprehensive educational lecture on the state of AI, Agents, and Robotics, PwC positions itself as the partner to help companies implement these technologies. They highlight that they offer end-to-end support—not just building the tech (PoC/Operation), but also the upfront strategy and the internal culture change (Education).
